{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Machine Translation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc8KZaJn3uBK"
      },
      "source": [
        "##**The Neural Translation Model (NMT)**\n",
        "\n",
        "For the NMT the network (a system of connected layers/models) used for training differs slightly from the network used for inference. Both use the the seq-to-seq encoder-decoder architecture. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4tsMpCYJUk1"
      },
      "source": [
        "###**The Inference Mode**\n",
        "\n",
        "**Encoder**\n",
        "\n",
        "The inference time encoding follows the same steps as training time encoding.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Decoder (No attention)**\n",
        "\n",
        "During training time, we passed a `batch_size(num_sents) x max_sentence_length` array representing the target words into the decoder lstm. The decoder_lstm learns how to represent a given target sentence using the context from the encoder lstm (that learns to represent a source sentence).  \n",
        "\n",
        "At test time, several things are different:\n",
        "\n",
        "1. We no longer have access to a complete translation of the source sentence (recall that no target_words array exists for dev and test sets). Rather, we initialize the target_words_array as thus:\n",
        "\n",
        "    Each expected sentence contains only a single token index, the index of the `'<start>'` token. So, the target_word_dev/test is a `batch_size x 1` array. (see the nmt.eval() function for this)\n",
        "\n",
        "2. This `batch_size x 1` array is fed to the trained decoder_lstm and the predicted array is a `batch_size x 1 x target_vocab_size` such that taking the argmax of this array accross the dimension 2 will give the most probable next word. \n",
        "\n",
        "    For example, at time_step `0`, the first time step, where the `step_target_words` given is the `batch_size x 1` array containing the `'<start>'` token, the next word prediction of the decoder is for each sentence (in the batch) the initial word in the sentence. \n",
        "\n",
        "3. At the first time step, the decoder_lstm still uses the encoder_states as it's initial states. At subsequent time steps, it uses it's own states from the previous time steps. This is also what the decoder_lstm does at training time but it is made more explicit here as we loop over time steps using a for loop.\n",
        "(see nmt.eval())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFIKOF5sed68"
      },
      "source": [
        "##**Training Without Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWDYU7YOgtBN",
        "outputId": "3dce1a34-13ad-4d56-ccfe-ccbc08086462"
      },
      "source": [
        "! pip install keras\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V5YoFP3yQO-",
        "outputId": "4c1a8df6-750e-4bef-a70c-6871c19a48d6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upXa_erqyzKm"
      },
      "source": [
        "# change this to the path to your folder. Remember to start from the home directory\n",
        "PATH = 'MyDrive/nmt_lab_files' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffbGcgrRy7p6"
      },
      "source": [
        "PATH_TO_FOLDER = \"/content/drive/\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfSgKakK0QgV"
      },
      "source": [
        "import sys\n",
        "sys.path.append(PATH_TO_FOLDER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS4hheAQ5-3_",
        "outputId": "aa9f4dcf-8913-47e4-ad90-504aed7f40ee"
      },
      "source": [
        "import nmt_model_keras\n",
        "import importlib\n",
        "importlib.reload(nmt_model_keras)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'nmt_model_keras' from '/content/nmt_model_keras.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmSgv44y0TN9"
      },
      "source": [
        "SOURCE_PATH = '/content/data.30.vi' \n",
        "TARGET_PATH = '/content/data.30.en' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOTMtc_l0_ut"
      },
      "source": [
        "import nmt_model_keras as nmt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsO7wW6U1w2m",
        "outputId": "34fd283d-fc6b-489b-fec2-0e750405f591"
      },
      "source": [
        "nmt.main(SOURCE_PATH, TARGET_PATH, use_attention=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading dictionaries\n",
            "read 24000/3000/3000 train/dev/test batches\n",
            "number of tokens in source: 2034, number of tokens in target:2506\n",
            "Task 1(a): Creating the embedding lookups...\n",
            "\n",
            "Task 1(b): Looking up source and target words...\n",
            "\n",
            "Task 1(c): Creating an encoder\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "\t\t\t\t\t\t Train Model Summary.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "source_embed_layer (Embedding)  (None, None, 100)    203400      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "target_embed_layer (Embedding)  (None, None, 100)    250600      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, None, 100)    0           source_embed_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, 100)    0           target_embed_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, None, 200),  240800      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  240800      dropout_1[0][0]                  \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 2506)   503706      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,439,306\n",
            "Trainable params: 1,439,306\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "\t\t\t\t\t\t Inference Time Encoder Model Summary.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "source_embed_layer (Embeddin (None, None, 100)         203400    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, None, 200), (None 240800    \n",
            "=================================================================\n",
            "Total params: 444,200\n",
            "Trainable params: 444,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            " Putting together the decoder states\n",
            "\t\t\t\t\t\t Decoder Inference Model summary\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "target_embed_layer (Embedding)  (None, None, 100)    250600      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, 100)    0           target_embed_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  240800      dropout_1[0][0]                  \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, None, 200)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 2506)   503706      lstm_1[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 995,106\n",
            "Trainable params: 995,106\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Starting training epoch 1/10\n",
            "240/240 [==============================] - 46s 53ms/step - loss: 2.7709 - accuracy: 0.5792\n",
            "Time used for epoch 1: 0 m 46 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 1.83\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 2/10\n",
            "240/240 [==============================] - 13s 53ms/step - loss: 1.8634 - accuracy: 0.6619\n",
            "Time used for epoch 2: 0 m 12 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 2.09\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 3/10\n",
            "240/240 [==============================] - 13s 53ms/step - loss: 1.7670 - accuracy: 0.6725\n",
            "Time used for epoch 3: 0 m 12 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 2.15\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 4/10\n",
            "240/240 [==============================] - 13s 53ms/step - loss: 1.6938 - accuracy: 0.6820\n",
            "Time used for epoch 4: 0 m 12 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 3.03\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 5/10\n",
            "240/240 [==============================] - 12s 52ms/step - loss: 1.6363 - accuracy: 0.6885\n",
            "Time used for epoch 5: 0 m 12 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 3.00\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 6/10\n",
            "240/240 [==============================] - 13s 53ms/step - loss: 1.5914 - accuracy: 0.6932\n",
            "Time used for epoch 6: 0 m 12 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 3.47\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 7/10\n",
            "240/240 [==============================] - 12s 52ms/step - loss: 1.5516 - accuracy: 0.6967\n",
            "Time used for epoch 7: 0 m 12 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 3.54\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 8/10\n",
            "240/240 [==============================] - 13s 53ms/step - loss: 1.5181 - accuracy: 0.6996\n",
            "Time used for epoch 8: 0 m 12 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 3.91\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 9/10\n",
            "240/240 [==============================] - 12s 52ms/step - loss: 1.4907 - accuracy: 0.7015\n",
            "Time used for epoch 9: 0 m 12 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 3.89\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 10/10\n",
            "240/240 [==============================] - 13s 52ms/step - loss: 1.4673 - accuracy: 0.7038\n",
            "Time used for epoch 10: 0 m 12 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 4.20\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Training finished!\n",
            "Time used for training: 3 m 26 s\n",
            "Evaluating on test set:\n",
            "source Sentence::trích dẫn thứ hai đến từ người đứng đầu cơ quan quản lý dịch vụ tài chính vương quốc anh . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Prediction::the <unk> of <unk> is <unk> <unk> , and the <unk> of <unk> <unk> .\n",
            "GroundTruth::the second quote is from the head of the u.k. financial services <unk> ./n\n",
            "source Sentence::chuyện trở nên tồi tệ hơn . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Prediction::the <unk> of <unk> .\n",
            "GroundTruth::it gets worse ./n\n",
            "source Sentence::chuyện gì đang diễn ra ở đây ? sao chuyện này lại có thể ? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Prediction::what about the <unk> ? why was the <unk> ?\n",
            "GroundTruth::what &apos;s happening here ? how can this be possible ?/n\n",
            "source Sentence::thật không may , câu trả lời là đúng vậy đấy . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Prediction::it &apos;s not a <unk> <unk> .\n",
            "GroundTruth::unfortunately , the answer is yes ./n\n",
            "source Sentence::nhưng mà , có một giải pháp rất thú vị đến từ lĩnh vực được biết đến như là một môn học của sự phức hợp . <pad> <pad>\n",
            "Prediction::but there &apos;s a <unk> <unk> <unk> <unk> <unk> <unk> , and it &apos;s a <unk> <unk> .\n",
            "GroundTruth::but there &apos;s an <unk> solution which is coming from what is known as the science of <unk> ./n\n",
            "Model BLEU score: 4.56\n",
            "Time used for evaluate on test set: 0 m 4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlMDC3DJi12c"
      },
      "source": [
        "##**Decoding with Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cQKwvFqurVY"
      },
      "source": [
        "The inputs to the attention layer are encoder and decoder outputs. The attention mechanism:\n",
        "1. Computes a score (a luong score) for each source word\n",
        "2. Weights the words by their luong scores.\n",
        "3. Concatenates the wieghted encoder representation with the decoder_ouput.\n",
        "This new decoder output will now be the input to the decoder_dense layer. \n",
        "\n",
        "Task 3 description in the doc file outlines the steps for this in detail. Once you have completed this Task, you are now ready to train with attention. Training time will be no more than 10 minutes using a GPU and you should get a bleu score of about 15."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23t6wfpLkb2F",
        "outputId": "0f257b8b-7394-4c4e-c65d-9e373661855d"
      },
      "source": [
        "nmt.main(SOURCE_PATH, TARGET_PATH, use_attention=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading dictionaries\n",
            "read 24000/3000/3000 train/dev/test batches\n",
            "number of tokens in source: 2034, number of tokens in target:2506\n",
            "Task 1(a): Creating the embedding lookups...\n",
            "\n",
            "Task 1(b): Looking up source and target words...\n",
            "\n",
            "Task 1(c): Creating an encoder\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "\t\t\t\t\t\t Train Model Summary.\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "source_embed_layer (Embedding)  (None, None, 100)    203400      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, None, 100)    0           source_embed_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "target_embed_layer (Embedding)  (None, None, 100)    250600      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 200),  240800      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, None, 100)    0           target_embed_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 200),  240800      dropout_3[0][0]                  \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer (None, None, 400)    0           lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 2506)   1004906     attention_layer[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 1,940,506\n",
            "Trainable params: 1,940,506\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "\t\t\t\t\t\t Inference Time Encoder Model Summary.\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "source_embed_layer (Embeddin (None, None, 100)         203400    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                [(None, None, 200), (None 240800    \n",
            "=================================================================\n",
            "Total params: 444,200\n",
            "Trainable params: 444,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\n",
            " Putting together the decoder states\n",
            "\t\t\t\t\t\t Decoder Inference Model summary\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "target_embed_layer (Embedding)  (None, None, 100)    250600      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, None, 100)    0           target_embed_layer[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_10 (InputLayer)           [(None, None, 200)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 200),  240800      dropout_3[0][0]                  \n",
            "                                                                 input_8[0][0]                    \n",
            "                                                                 input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer_1 (AttentionLay (None, None, 400)    0           input_10[0][0]                   \n",
            "                                                                 lstm_3[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 2506)   1004906     attention_layer_1[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 1,496,306\n",
            "Trainable params: 1,496,306\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Starting training epoch 1/10\n",
            "240/240 [==============================] - 17s 58ms/step - loss: 2.9297 - accuracy: 0.5489\n",
            "Time used for epoch 1: 0 m 16 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 2.00\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 2/10\n",
            "240/240 [==============================] - 14s 58ms/step - loss: 1.8044 - accuracy: 0.6751\n",
            "Time used for epoch 2: 0 m 13 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 6.77\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 3/10\n",
            "240/240 [==============================] - 14s 57ms/step - loss: 1.4456 - accuracy: 0.7259\n",
            "Time used for epoch 3: 0 m 13 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 11.70\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 4/10\n",
            "240/240 [==============================] - 14s 57ms/step - loss: 1.2234 - accuracy: 0.7511\n",
            "Time used for epoch 4: 0 m 13 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 14.02\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 5/10\n",
            "240/240 [==============================] - 14s 56ms/step - loss: 1.0990 - accuracy: 0.7652\n",
            "Time used for epoch 5: 0 m 13 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 15.04\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 6/10\n",
            "240/240 [==============================] - 14s 57ms/step - loss: 1.0189 - accuracy: 0.7749\n",
            "Time used for epoch 6: 0 m 13 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 15.43\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 7/10\n",
            "240/240 [==============================] - 14s 57ms/step - loss: 0.9605 - accuracy: 0.7826\n",
            "Time used for epoch 7: 0 m 13 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 15.25\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 8/10\n",
            "240/240 [==============================] - 14s 57ms/step - loss: 0.9189 - accuracy: 0.7879\n",
            "Time used for epoch 8: 0 m 13 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 15.77\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 9/10\n",
            "240/240 [==============================] - 14s 58ms/step - loss: 0.8822 - accuracy: 0.7933\n",
            "Time used for epoch 9: 0 m 13 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 15.60\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Starting training epoch 10/10\n",
            "240/240 [==============================] - 14s 57ms/step - loss: 0.8545 - accuracy: 0.7972\n",
            "Time used for epoch 10: 0 m 13 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 15.29\n",
            "Time used for evaluate on dev set: 0 m 5 s\n",
            "Training finished!\n",
            "Time used for training: 3 m 12 s\n",
            "Evaluating on test set:\n",
            "source Sentence::trích dẫn thứ hai đến từ người đứng đầu cơ quan quản lý dịch vụ tài chính vương quốc anh . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Prediction::the second thing that <unk> two comes from the first pair of <unk> <unk> <unk> .\n",
            "GroundTruth::the second quote is from the head of the u.k. financial services <unk> ./n\n",
            "source Sentence::chuyện trở nên tồi tệ hơn . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Prediction::and it becomes worse .\n",
            "GroundTruth::it gets worse ./n\n",
            "source Sentence::chuyện gì đang diễn ra ở đây ? sao chuyện này lại có thể ? <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Prediction::what is going on here ? why can this be ?\n",
            "GroundTruth::what &apos;s happening here ? how can this be possible ?/n\n",
            "source Sentence::thật không may , câu trả lời là đúng vậy đấy . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Prediction::it &apos;s not true , the answer is true .\n",
            "GroundTruth::unfortunately , the answer is yes ./n\n",
            "source Sentence::nhưng mà , có một giải pháp rất thú vị đến từ lĩnh vực được biết đến như là một môn học của sự phức hợp . <pad> <pad>\n",
            "Prediction::but , there &apos;s a very interesting solution to do is that it &apos;s fun to know that &apos;s a <unk> of <unk> .\n",
            "GroundTruth::but there &apos;s an <unk> solution which is coming from what is known as the science of <unk> ./n\n",
            "Model BLEU score: 15.85\n",
            "Time used for evaluate on test set: 0 m 5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUOUAjLYpCMk"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIPNBStG2wTV"
      },
      "source": [
        "my_table  = PrettyTable([\" Model_Type\", \"Blue_Score\",\"Epochs\"])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m1IGL9K27lI"
      },
      "source": [
        "my_table.add_row([\"NMT without Attention\",\"4.8\",\"10\"])\n",
        "my_table.add_row([\"NMT with Attention\",\"15.6\",\"10\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD-f07ZN29ww",
        "outputId": "b75159be-9ed4-48a6-afa2-b0842e89cbf1"
      },
      "source": [
        "print(my_table)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------+------------+--------+\n",
            "|       Model_Type      | Blue_Score | Epochs |\n",
            "+-----------------------+------------+--------+\n",
            "| NMT without Attention |    4.8     |   10   |\n",
            "|   NMT with Attention  |    15.6    |   10   |\n",
            "+-----------------------+------------+--------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItUu8msc3YYR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}